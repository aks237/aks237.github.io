---
layout: project
title: "Autonomous Navigation"
---
<h6>
  ROS2, Python, Gazebo, YOLOv8, Perception, Sensor Fusion, Localization, Planning and Control
</h6>

<h5>Overview</h5>

<p>This is the work I have done so far in the 2024-2025 year leading the software development to
  complete the autonomous mission in the annual University Rover Competition for Cornell’s Mars
  Rover project team. The mission requires that the robot autonomously navigate through the desert
  terrain in Utah to 7 GNSS coordinates, avoiding obstacles and performing various detection tasks
  upon arrival at coordinates,. Teams are given a coordinates at the time of the mission, including the starting
  coordinate. The first 2 coordinates the rover must simply drive within 3m of, the
  next 3 coordinates the rover must locate an AR tag on a signpost within a 20m radius and navigate
  within 2m of, and the last 2 coordinates the rover must identify and drive within 2m of a small object
  on the ground. While no software existed for this mission at the start of this year, the rover used
  to compete in last year's competition was assembled and ready to be used as a testing platform while the
  new rover for this upcoming year’s competition is being designed and built.
</p>

<div class="project-images">
  <div class="project-image">
    <iframe src="https://drive.google.com/file/d/1R0-3mOJ33xw-Oh0DX_xlMnWDw05o0yKl/preview" width="1280" height="720"
      allow="autoplay" allowfullscreen>
    </iframe>
  </div>
</div>

<h5>High Level Software Diagram</h5>
<div class="project-image">
  <img src="{{ '/assets/images/CMR/autonomy/navigation_flowchart.png' | relative_url }}" alt="">
</div>

<p>
</p>

<h5>Simulation Environment</h5>

<p>
  To enable rapid testing
  of software developed for this mission, I built a simulation in Gazebo that
  interfaces with ROS (robot operating system), to test all functionalities of this mission. This simulation
  includes the controls for the swerve drive system the rover uses and the data (including noise) from
  the IMU, GPS, and depth camera sensors utilized in this mission. In addition, the
  simulation environment contains randomly generated terrains resembling the Utah desert including hills,
  rocks, bushes, and the objects specific to the autonomy mission.
</p>

<div class="project-images">
  <!-- Sim terrain screenshot -->
  <div class="project-image">
    <img src="{{ '/assets/images/CMR/autonomy/sim-screenshot.jpg' | relative_url }}" alt="">
    <p class="image-caption">Simulation terrain - cylinders help visualize target GNSS coordinates</p>
  </div>

  <!-- Sim AR Tags screenshot -->
  <div class="project-image">
    <img src="{{ '/assets/images/CMR/autonomy/ar_tags_sim.png' | relative_url }}" alt="">
    <p class="image-caption">Mission AR tags</p>
  </div>
</div>

<!-- Sim Terrain Video with RVIZ -->
<div>
  <div class="project-image">
    <iframe src="https://drive.google.com/file/d/1F11CnuS1OFJkzDSA7mzF0BHSvY62vFPS/preview" width="1280" height="720"
      allow="autoplay" allowfullscreen>
    </iframe>
  </div>
</div>

<p>
</p>

<h5>State Machine</h5>

<p>
  The state machine is the central node of the autonomy software, managing the rover's state and transitioning between
  states when the current mission objective has been accomplished. The node sends target positions to the path
  planner and receives the current position of the rover from the localization node.
</p>

<p>
  Full writeup coming soon!
</p>

<h5>Localization</h5>

<p>
  The rover has an RTK GPS system to localize itself, utilizing correction signals sent by the basestation via TCP to
  the rover to achieve a GPS reading accurate to within one centimeter. The orientation of the rover is determined by
  the onboard IMU.
</p>

<p>
  Full writeup coming soon!
</p>

<h5>Path Planning</h5>

<div class="project-images">
  <!-- Path planning screenshot -->
  <div class="project-image">
    <img src="{{ '/assets/images/CMR/autonomy/path_planning.png' | relative_url }}" alt="">
  </div>
</div>
<p>
  The path planning node receives a target position from the state machine and uses the A* algorithm to plan a path of
  waypoints on the current global costmap.
</p>
<p>
  Full writeup coming soon!
</p>

<h5>Control</h5>

<p>
  The control node sends drive commands to the swerve drive system utilizing a Stanley controller to follow the planned
  path. Occasionally, the point turning ability of the swerve drive system is used when sharp turns are needed.
</p>

<p>
  Full writeup coming soon!
</p>

<h5>Costmap Generation</h5>

<p>
  The costmap node stores the current global costmap and updates it, fusing the latest data from the localization node
  and the ZED 2i stereo camera. The ZED's point cloud data and built-in ground plane detection feature are used to
  determine traversable regions based on the current line of sight of the rover. An update of the current global costmap
  is sent to the path planner node at a frequency of 10 Hz.
</p>

<p>
  Full writeup coming soon!
</p>
<h5>Future Improvements</h5>

<p>
</p>

<h5>Acknowledgements</h5>

<p>
</p>